{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Healthcare_chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fObc5UGH3Pwu",
        "outputId": "75813f93-ce24-4ca3-ae63-1ed7fce8ad02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G35-e8HJekJE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ea58dc5-8978-4945-e966-adba9f13ab47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from io import StringIO\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import json\n",
        "import csv\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "import joblib\n",
        "import pickle\n",
        "import nltk \n",
        "nltk.download('punkt')\n",
        "import tensorflow as tf \n",
        "from tensorflow.keras.layers import Dense\n",
        "import random\n",
        "from nltk.stem.lancaster import LancasterStemmer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_precaution(text): \n",
        "  disease = get_disease(text)\n",
        "  data = pd.read_csv('drive/MyDrive/disymp/symptom_precaution.csv')\n",
        "  data['Disease'] = data['Disease'].apply(lambda x: x.lower())\n",
        "  data['Disease'] = data['Disease'].apply(lambda x:x.strip())\n",
        "  precaution = data[data['Disease'] == disease].values\n",
        "  prec=[]\n",
        "  if(precaution.shape[0] != 0):\n",
        "    for i in range(1,precaution.shape[1]):\n",
        "      if type(precaution[0][i]) == str:\n",
        "        prec.append(precaution[0][i])\n",
        "  return prec"
      ],
      "metadata": {
        "id": "r4f1teps3sdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_description(text):\n",
        "  disease = get_disease(text)\n",
        "  data = pd.read_csv('drive/MyDrive/disymp/symptom_Description.csv')\n",
        "  data['Disease'] = data['Disease'].apply(lambda x: x.lower())\n",
        "  data['Disease'] = data['Disease'].apply(lambda x:x.strip())\n",
        "  description = data[data['Disease'] == disease].values\n",
        "  desc = None\n",
        "  if description.shape[0]!=0:\n",
        "    desc = description[0][1]\n",
        "  return desc"
      ],
      "metadata": {
        "id": "Vdl-g_FYgbtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_symptoms(text): \n",
        "  symptoms = pd.read_csv('drive/MyDrive/disymp/symptoms.csv')\n",
        "  symp_list = list(symptoms['Symptoms'].values)\n",
        "  symps = []\n",
        "  for i in symp_list: \n",
        "    if(text.find(i)!=-1):\n",
        "      symps.append(i)\n",
        "  return symps"
      ],
      "metadata": {
        "id": "29FNFu1EgooI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_intent(text):\n",
        "\n",
        "  with open('drive/MyDrive/disymp/intentcla.pkl', 'rb') as f:\n",
        "    count_vect, clf = pickle.load(f)  \n",
        "  intent = clf.predict(count_vect.transform([text]))\n",
        "  return str(intent).strip(\"['']\")"
      ],
      "metadata": {
        "id": "e2xMUQY6gsfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sympvector(text):\n",
        "  symps = get_symptoms(text)\n",
        "  sympid = pd.read_csv('drive/MyDrive/disymp/symptoms.csv')\n",
        "  sympid.columns = ['idx','Symptoms']\n",
        "  sympid.set_index('Symptoms')\n",
        "  indexes = []\n",
        "  for i in symps:\n",
        "    indexes.append(sympid[sympid['Symptoms'] == i]['idx'].values[0])\n",
        "  vec = [0]*131\n",
        "  for i in range(len(indexes)): \n",
        "    vec[indexes[i]] = 1\n",
        "  return np.array(vec)"
      ],
      "metadata": {
        "id": "quCeFl95g5st"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yAMjK9QoGEWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_disease(text):\n",
        "  vec = get_sympvector(text).reshape(1,131)\n",
        "  classifier = joblib.load('drive/MyDrive/disymp/discla.pkl')\n",
        "  return classifier.predict(vec)[0]"
      ],
      "metadata": {
        "id": "w--nfo8nhw-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_disease(text): \n",
        "  dis = pd.read_csv('drive/MyDrive/disymp/disease.csv')\n",
        "  dislist = list(dis['Diseases'].values)\n",
        "  disease = ''\n",
        "  for i in dislist:\n",
        "    if text.find(i)!=-1:\n",
        "      disease = i\n",
        "      break\n",
        "  return disease"
      ],
      "metadata": {
        "id": "lQrRKfp_6fal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response(text):\n",
        "  stemmer = LancasterStemmer()\n",
        "  with open('drive/MyDrive/disymp/diaintents.json') as json_data: \n",
        "    intents = json.load(json_data)\n",
        "  words = []\n",
        "  classes = []\n",
        "  documents = []\n",
        "  ignore_words = ['?']\n",
        "  for intent in intents['intents']:\n",
        "      for pattern in intent['patterns']:\n",
        "\n",
        "          w = nltk.word_tokenize(pattern)\n",
        "\n",
        "          words.extend(w)\n",
        "\n",
        "          documents.append((w, intent['tag']))\n",
        "\n",
        "          if intent['tag'] not in classes:\n",
        "              classes.append(intent['tag'])\n",
        "\n",
        "  words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
        "  words = sorted(list(set(words)))\n",
        "\n",
        "  classes = sorted(list(set(classes)))\n",
        "\n",
        "  def clean_up_sentence(sentence):\n",
        "\n",
        "      sentence_words = nltk.word_tokenize(sentence)\n",
        "\n",
        "      sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
        "      return sentence_words\n",
        "\n",
        "  def bow(sentence, words, show_details=False):\n",
        "\n",
        "      sentence_words = clean_up_sentence(sentence)\n",
        "\n",
        "      bag = [0]*len(words)  \n",
        "      for s in sentence_words:\n",
        "          for i,w in enumerate(words):\n",
        "              if w == s: \n",
        "                  bag[i] = 1\n",
        "                  if show_details:\n",
        "                      print (\"found in bag: %s\" % w)\n",
        "\n",
        "      return(np.array(bag))\n",
        "  context = {}\n",
        "\n",
        "  ERROR_THRESHOLD = 0.25\n",
        "  def classify(sentence):\n",
        "      model = tf.keras.models.load_model('drive/MyDrive/disymp/diamodel.h5')\n",
        "      results = model.predict([bow(sentence, words).reshape(1,47)])[0]\n",
        "      results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
        "      results.sort(key=lambda x: x[1], reverse=True)\n",
        "      return_list = []\n",
        "      for r in results:\n",
        "          return_list.append((classes[r[0]], r[1]))\n",
        "      return return_list\n",
        "\n",
        "  def response(sentence, userID='123', show_details=False):\n",
        "      results = classify(sentence)\n",
        "\n",
        "      if results:\n",
        "\n",
        "          while results:\n",
        "              for i in intents['intents']:\n",
        "\n",
        "                  if i['tag'] == results[0][0]:\n",
        "\n",
        "                      if 'context_set' in i:\n",
        "                          if show_details: print ('context:', i['context_set'])\n",
        "                          context[userID] = i['context_set']\n",
        "\n",
        "                      if not 'context_filter' in i or \\\n",
        "                          (userID in context and 'context_filter' in i and i['context_filter'] == context[userID]):\n",
        "                          if show_details: print ('tag:', i['tag'])\n",
        "                          return random.choice(i['responses'])\n",
        "\n",
        "              results.pop(0)\n",
        "  return response(text) "
      ],
      "metadata": {
        "id": "s_R4wqHAWFuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True: \n",
        "  user_text = input('User: ')\n",
        "  user_text = user_text.lower()\n",
        "  intent = get_intent(user_text)\n",
        "  if(user_text == 'hello'):\n",
        "    print(\"Chatbot: hello\")\n",
        "  elif user_text=='exit':\n",
        "    break \n",
        "  elif intent == 'disease_descritpion':\n",
        "    print('Chatbot: ',get_description(user_text))\n",
        "  elif intent == 'disease_precaution': \n",
        "    reply = get_precaution(user_text)\n",
        "    for i in reply:\n",
        "      print('Chatbot: ',i)\n",
        "  elif intent == \"symptoms_checker\":\n",
        "    print(\"Chatbot: You are diagonsed with: \",classify_disease(user_text))\n",
        "    if(classify_disease(user_text) == 'Diabetes '):\n",
        "      print('Chatbot: Please enter query if you would like to know more about diabetes or else type exit: ')\n",
        "      text = input(\"User: \")\n",
        "      if text == 'exit':\n",
        "        break\n",
        "      else: \n",
        "        print('Chatbot: ',get_response(text))"
      ],
      "metadata": {
        "id": "WmG2SfnF6fvW",
        "outputId": "b9cf374a-227b-46a3-ed10-46fc2da85ae7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: hello\n",
            "Chatbot: hello\n",
            "User: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WOMeFJIciTkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UFpA7Fxh-0YA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7eRbVpZD_PLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qwNFGXSyReOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Rz_vt2w6ReEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RjtLPpW3Rd6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kYfCj7kvRdwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mqYdXlxVRdkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lKkieNpafDuX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}